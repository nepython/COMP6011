{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49578faa-cb26-4b5d-a3d3-a941ab45b658",
   "metadata": {},
   "source": [
    "## Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec074e3e-5c98-4041-a56e-025f055e23ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DL_ECG_Classification'...\n",
      "remote: Enumerating objects: 811, done.\u001b[K\n",
      "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
      "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
      "remote: Total 811 (delta 162), reused 204 (delta 127), pack-reused 552 (from 1)\u001b[K\n",
      "Receiving objects: 100% (811/811), 14.00 MiB | 16.55 MiB/s, done.\n",
      "Resolving deltas: 100% (501/501), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/HemaxiN/DL_ECG_Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3885f8f0-5897-4360-b974-72645d1b2b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting argon2-cffi==21.3.0 (from -r DL_ECG_Classification/requirements.txt (line 1))\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting argon2-cffi-bindings==21.2.0 (from -r DL_ECG_Classification/requirements.txt (line 2))\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting asttokens==2.0.5 (from -r DL_ECG_Classification/requirements.txt (line 3))\n",
      "  Using cached asttokens-2.0.5-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting attrs==21.4.0 (from -r DL_ECG_Classification/requirements.txt (line 4))\n",
      "  Using cached attrs-21.4.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.10.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: bleach==4.1.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 7)) (4.1.0)\n",
      "Collecting certifi==2021.10.8 (from -r DL_ECG_Classification/requirements.txt (line 8))\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: cffi==1.15.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 9)) (1.15.0)\n",
      "Collecting charset-normalizer==2.0.12 (from -r DL_ECG_Classification/requirements.txt (line 10))\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama==0.4.4 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 11)) (0.4.4)\n",
      "Requirement already satisfied: cycler==0.11.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 12)) (0.11.0)\n",
      "Collecting debugpy==1.6.0 (from -r DL_ECG_Classification/requirements.txt (line 13))\n",
      "  Using cached debugpy-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting decorator==5.1.1 (from -r DL_ECG_Classification/requirements.txt (line 14))\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 15)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 16)) (0.4)\n",
      "Collecting executing==0.8.3 (from -r DL_ECG_Classification/requirements.txt (line 17))\n",
      "  Using cached executing-0.8.3-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting fonttools==4.31.2 (from -r DL_ECG_Classification/requirements.txt (line 18))\n",
      "  Using cached fonttools-4.31.2-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: idna==3.3 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 19)) (3.3)\n",
      "Collecting importlib-resources==5.6.0 (from -r DL_ECG_Classification/requirements.txt (line 20))\n",
      "  Using cached importlib_resources-5.6.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting ipykernel==6.10.0 (from -r DL_ECG_Classification/requirements.txt (line 21))\n",
      "  Using cached ipykernel-6.10.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipython==8.2.0 (from -r DL_ECG_Classification/requirements.txt (line 22))\n",
      "  Using cached ipython-8.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 23)) (0.2.0)\n",
      "Collecting ipywidgets==7.7.0 (from -r DL_ECG_Classification/requirements.txt (line 24))\n",
      "  Using cached ipywidgets-7.7.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jedi==0.18.1 (from -r DL_ECG_Classification/requirements.txt (line 25))\n",
      "  Using cached jedi-0.18.1-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting Jinja2==3.1.1 (from -r DL_ECG_Classification/requirements.txt (line 26))\n",
      "  Using cached Jinja2-3.1.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting joblib==1.1.0 (from -r DL_ECG_Classification/requirements.txt (line 27))\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jsonschema==4.4.0 (from -r DL_ECG_Classification/requirements.txt (line 28))\n",
      "  Using cached jsonschema-4.4.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting jupyter==1.0.0 (from -r DL_ECG_Classification/requirements.txt (line 29))\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
      "Collecting jupyter-client==7.1.2 (from -r DL_ECG_Classification/requirements.txt (line 30))\n",
      "  Using cached jupyter_client-7.1.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting jupyter-console==6.4.3 (from -r DL_ECG_Classification/requirements.txt (line 31))\n",
      "  Using cached jupyter_console-6.4.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jupyter-core==4.9.2 (from -r DL_ECG_Classification/requirements.txt (line 32))\n",
      "  Using cached jupyter_core-4.9.2-py3-none-any.whl.metadata (870 bytes)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 33)) (0.1.2)\n",
      "Collecting jupyterlab-widgets==1.1.0 (from -r DL_ECG_Classification/requirements.txt (line 34))\n",
      "  Using cached jupyterlab_widgets-1.1.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting kiwisolver==1.4.2 (from -r DL_ECG_Classification/requirements.txt (line 35))\n",
      "  Using cached kiwisolver-1.4.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting llvmlite==0.38.1 (from -r DL_ECG_Classification/requirements.txt (line 36))\n",
      "  Using cached llvmlite-0.38.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting MarkupSafe==2.1.1 (from -r DL_ECG_Classification/requirements.txt (line 37))\n",
      "  Using cached MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting matplotlib==3.5.1 (from -r DL_ECG_Classification/requirements.txt (line 38))\n",
      "  Using cached matplotlib-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 39)) (0.1.3)\n",
      "Collecting mistune==0.8.4 (from -r DL_ECG_Classification/requirements.txt (line 40))\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbclient==0.5.13 (from -r DL_ECG_Classification/requirements.txt (line 41))\n",
      "  Using cached nbclient-0.5.13-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting nbconvert==6.4.5 (from -r DL_ECG_Classification/requirements.txt (line 42))\n",
      "  Using cached nbconvert-6.4.5-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting nbformat==5.2.0 (from -r DL_ECG_Classification/requirements.txt (line 43))\n",
      "  Using cached nbformat-5.2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.5.4 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 44)) (1.5.4)\n",
      "Collecting notebook==6.4.10 (from -r DL_ECG_Classification/requirements.txt (line 45))\n",
      "  Using cached notebook-6.4.10-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting numba==0.55.2 (from -r DL_ECG_Classification/requirements.txt (line 46))\n",
      "  Using cached numba-0.55.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting numpy==1.22.3 (from -r DL_ECG_Classification/requirements.txt (line 47))\n",
      "  Using cached numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting opencv-python==4.5.5.64 (from -r DL_ECG_Classification/requirements.txt (line 48))\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting packaging==21.3 (from -r DL_ECG_Classification/requirements.txt (line 49))\n",
      "  Using cached packaging-21.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pandas==1.5.2 (from -r DL_ECG_Classification/requirements.txt (line 50))\n",
      "  Using cached pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 51)) (1.5.0)\n",
      "Collecting parso==0.8.3 (from -r DL_ECG_Classification/requirements.txt (line 52))\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 53)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.0.1 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 54)) (9.0.1)\n",
      "Collecting prettytable==3.3.0 (from -r DL_ECG_Classification/requirements.txt (line 55))\n",
      "  Using cached prettytable-3.3.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting prometheus-client==0.13.1 (from -r DL_ECG_Classification/requirements.txt (line 56))\n",
      "  Using cached prometheus_client-0.13.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.28 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 57)) (3.0.28)\n",
      "Requirement already satisfied: psutil==5.9.0 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 58)) (5.9.0)\n",
      "Collecting pure-eval==0.2.2 (from -r DL_ECG_Classification/requirements.txt (line 59))\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pycparser==2.21 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 60)) (2.21)\n",
      "Collecting Pygments==2.11.2 (from -r DL_ECG_Classification/requirements.txt (line 61))\n",
      "  Using cached Pygments-2.11.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyparsing==3.0.7 (from -r DL_ECG_Classification/requirements.txt (line 62))\n",
      "  Using cached pyparsing-3.0.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyrsistent==0.18.1 in /usr/lib/python3/dist-packages (from -r DL_ECG_Classification/requirements.txt (line 63)) (0.18.1)\n",
      "Collecting python-dateutil==2.8.2 (from -r DL_ECG_Classification/requirements.txt (line 64))\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting pyts==0.12.0 (from -r DL_ECG_Classification/requirements.txt (line 65))\n",
      "  Using cached pyts-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytz==2022.7.1 (from -r DL_ECG_Classification/requirements.txt (line 66))\n",
      "  Using cached pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==303 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pywin32==303\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -r DL_ECG_Classification/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c49151a-b881-4308-9a63-1dc5bcfd67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tifffile pyts wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73053b3d-2d44-4730-9f8f-7118a106b91e",
   "metadata": {},
   "source": [
    "## Setup PTB-XL dataset (v1.0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac130a13-a9ab-47fc-ba46-d80acaef607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-13 11:52:08--  https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1839504686 (1.7G) [application/zip]\n",
      "Saving to: ‘ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip’\n",
      "\n",
      " ptb-xl-a-large-pub  92%[=================>  ]   1.59G   334KB/s    eta 6m 35s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d7823-69bb-4b31-ae6b-97586e587c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip -d p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de65b50c-e8e7-47ae-ac2a-3b87e6b0b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "path = 'ptb-xl-1.0.3/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'\n",
    "sampling_rate=100\n",
    "\n",
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Load raw signal data\n",
    "X = load_raw_data(Y, sampling_rate, path)\n",
    "\n",
    "# Load scp_statements.csv for diagnostic aggregation\n",
    "agg_df = pd.read_csv(path+'scp_statements.csv', index_col=0)\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            if key in ['CLBBB', 'ILBBB']:\n",
    "                key = 'LBBB'\n",
    "            elif key in ['CRBBB', 'IRBBB']:\n",
    "                key = 'RBBB'\n",
    "            elif key == '1AVB':\n",
    "                key = '1dAVB'\n",
    "            if key in ['AFIB', 'AFLT', '1dAVb', 'RBBB', 'LBBB']:\n",
    "                tmp.append(key)\n",
    "    return list(set(tmp))\n",
    "\n",
    "# Apply diagnostic superclass\n",
    "Y['subclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n",
    "Y = Y[Y.subclass.apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Split data into train and test (https://physionet.org/content/ptb-xl/1.0.1/)\n",
    "test_fold = 10\n",
    "dev_fold = 9\n",
    "# Train: 2844 records\n",
    "X_train = X[np.where((Y.strat_fold != test_fold) & (Y.strat_fold != dev_fold))]\n",
    "y_train = Y[(Y.strat_fold != test_fold) & (Y.strat_fold != dev_fold)].subclass\n",
    "# Validation: 357 records\n",
    "X_dev = X[np.where(Y.strat_fold == dev_fold)]\n",
    "y_dev = Y[(Y.strat_fold == dev_fold)].subclass\n",
    "# Test: 362 records\n",
    "X_test = X[np.where(Y.strat_fold == test_fold)]\n",
    "y_test = Y[Y.strat_fold == test_fold].subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "819cbe30-6b9b-420b-b544-b4dae03cb301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFIB',\n",
       " 'AFIB,AFLT',\n",
       " 'AFIB,LBBB',\n",
       " 'AFIB,RBBB',\n",
       " 'AFLT',\n",
       " 'AFLT,RBBB',\n",
       " 'LBBB',\n",
       " 'LBBB,AFLT',\n",
       " 'LBBB,RBBB',\n",
       " 'RBBB'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conditions covered\n",
    "set([','.join(r) for r in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9eea17f-1e67-4773-b6c0-9b455be6722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dir = 'Processed'\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X.pickle\"),\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y.pickle\"),\"wb\")\n",
    "pickle.dump(Y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_train.pickle\"),\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_train.pickle\"),\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_dev.pickle\"),\"wb\")\n",
    "pickle.dump(X_dev, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_dev.pickle\"),\"wb\")\n",
    "pickle.dump(y_dev, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_test.pickle\"),\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_test.pickle\"),\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6cc5cd3-40c9-4191-aa69-26c5be8e16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelstovector(X,y):\n",
    "  '''\n",
    "  Convert the labels in y into vectors:\n",
    "  Multi-label problem:\n",
    "  AFIB: [0,0,0,0]\n",
    "  AFLT: [1,0,0,0]\n",
    "  1dAVb: [0,1,0,0]\n",
    "  RBBB: [0,0,1,0]\n",
    "  LBBB: [0,0,0,1]\n",
    "  Combination example:\n",
    "  AFLT and LBBB: [1,0,0,1]\n",
    "  LBBB and RBBB and 1dAVB: [0,1,1,1]\n",
    "  -----------------------------------------------------------\n",
    "  Args: X (number of examples, signal length, number of leads)\n",
    "        y (number of examples, )\n",
    "  '''\n",
    "  y_list = []\n",
    "  X_list = []\n",
    "  for label, ecg in zip(y,X):\n",
    "    if len(label)!=0: #ignore examples with label = []\n",
    "      aux_vec = np.zeros(4)\n",
    "      if 'AFLT' in label:\n",
    "        aux_vec[0] = 1\n",
    "      if '1dAVB' in label:\n",
    "        aux_vec[1] = 1\n",
    "      if 'RBBB' in label:\n",
    "        aux_vec[2] = 1\n",
    "      if 'LBBB' in label:\n",
    "        aux_vec[3] = 1\n",
    "\n",
    "      y_list.append(aux_vec)\n",
    "      X_list.append(ecg)\t\n",
    "\n",
    "  return X_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc6d0d4c-f6f8-4af8-8f9d-e89bb124d410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2844, 1000, 12)\n",
      "(357, 1000, 12)\n",
      "(362, 1000, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train_processed, y_train_processed = labelstovector(X_train, y_train)\n",
    "X_dev_processed, y_dev_processed = labelstovector(X_dev, y_dev)\n",
    "X_test_processed, y_test_processed = labelstovector(X_test, y_test)\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_train_processed.pickle\"),\"wb\")\n",
    "pickle.dump(X_train_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_train_processed.pickle\"),\"wb\")\n",
    "pickle.dump(y_train_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_dev_processed.pickle\"),\"wb\")\n",
    "pickle.dump(X_dev_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_dev_processed.pickle\"),\"wb\")\n",
    "pickle.dump(y_dev_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"X_test_processed.pickle\"),\"wb\")\n",
    "pickle.dump(X_test_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(os.path.join(preprocessed_dir, \"y_test_processed.pickle\"),\"wb\")\n",
    "pickle.dump(y_test_processed, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c869a9d6-494a-44ad-a08f-048f85f030d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Prepare 2D dataset\n",
    "!mkdir -p Images/dev/labels\n",
    "!mkdir -p Images/dev/images\n",
    "!mkdir -p Images/train/labels\n",
    "!mkdir -p Images/train/images\n",
    "!mkdir -p Images/test/labels\n",
    "!mkdir -p Images/test/images\n",
    "# Save processed dataset at\n",
    "!mkdir -p Processed/model_specific/X_rnn_train\n",
    "!mkdir -p Processed/model_specific/X_rnn_dev\n",
    "!mkdir -p Processed/model_specific/X_rnn_test\n",
    "!mkdir -p Processed/model_specific/X_cnn_train\n",
    "!mkdir -p Processed/model_specific/X_cnn_dev\n",
    "!mkdir -p Processed/model_specific/X_cnn_test\n",
    "!mkdir -p Processed/model_specific/labels_train\n",
    "!mkdir -p Processed/model_specific/labels_dev\n",
    "!mkdir -p Processed/model_specific/labels_test\n",
    "\n",
    "# Modify `create_dataset.py` appropriately to\n",
    "# specify dataset path and save directory path\n",
    "!python DL_ECG_Classification/Dataset/create_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e09427-a980-4f2a-afb8-4ab96a9d6cda",
   "metadata": {},
   "source": [
    "## Training\n",
    "Update code for specific models to ensure dataset length is correctly specified:\n",
    "```python\n",
    "# _examples_ = [17111,2156,2163]\n",
    "_examples_ = [2844,357,362]\n",
    "```\n",
    "\n",
    "Matrix printed at the end contains the column: TP, FN, FP, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021fbcf-44f6-4473-ba36-7db54f3f5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D models\n",
    "## RNN\n",
    "!python3 DL_ECG_Classification/rnn.py -data 'Processed/model_specific/' -epochs 100 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01\n",
    "## LSTM\n",
    "!python3 DL_ECG_Classification/lstm.py -data 'Processed/model_specific/' -epochs 100 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01\n",
    "## GRU\n",
    "!python3 DL_ECG_Classification/gru.py -data 'Processed/model_specific/' -epochs 100 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ed09d1a-f018-4d13-b201-f816bfeabb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nepython/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "Loading data...\n",
      "Training epoch 1\n",
      "/home/nepython/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/nepython/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training loss: 2.4530\n",
      "Training epoch 2\n",
      "Training loss: 2.4164\n",
      "Final Test Results:\n",
      "/home/nepython/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1188: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/home/nepython/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/nepython/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1188: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n",
      "[[  0.   7.   0. 355.]\n",
      " [  0.   0. 362.   0.]\n",
      " [  0. 166.   0. 196.]\n",
      " [ 62.   0. 300.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# 2D models\n",
    "## AlexNet\n",
    "!python3 DL_ECG_Classification/AlexNet.py -data 'Processed/model_specific/' -epochs 2 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01\n",
    "# ## VGGNet\n",
    "# !python3 DL_ECG_Classification/vggnet.py -data 'Processed/model_specific/' -epochs 100 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01\n",
    "# ## ResNet\n",
    "# !python3 DL_ECG_Classification/resnet.py -data 'Processed/model_specific/' -epochs 100 -batch_size 256 -path_save_model 'saved_models/' -gpu_id 0 -learning_rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4f4fa0f-fa63-4d69-911a-6597f7289d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'0010': 1323, '0001': 487, '0000': 974, '1000': 55, '1001': 2, '1010': 2, '0011': 1}\n",
      "\n",
      "dev\n",
      "{'0010': 166, '0000': 123, '0001': 61, '1000': 7}\n",
      "\n",
      "test\n",
      "{'0010': 166, '0000': 127, '0001': 62, '1000': 7}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# View distribution\n",
    "train_dir = 'Processed/model_specific/labels_train/'\n",
    "dev_dir = 'Processed/model_specific/labels_dev/'\n",
    "test_dir = 'Processed/model_specific/labels_test/'\n",
    "\n",
    "for path in [train_dir, dev_dir, test_dir]:\n",
    "    counter = dict()\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        vector = np.load(os.path.join(path, file))\n",
    "        encoding = ''.join(vector.astype(int).astype(str))\n",
    "        if encoding not in counter:\n",
    "            counter[encoding] = 0\n",
    "        counter[encoding] += 1\n",
    "    print(path.split('/')[-2].split('_')[-1])\n",
    "    print(counter)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
